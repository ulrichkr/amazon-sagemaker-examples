{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"sagemaker==2.163.0\" huggingface_hub safetensors loguru --upgrade --quiet --index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint huggingface-pytorch-tgi-inference-2023-06-14-12-16-29-069: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     19\u001b[0m huggingface_model \u001b[39m=\u001b[39m HuggingFaceModel(\n\u001b[1;32m     20\u001b[0m \timage_uri\u001b[39m=\u001b[39mget_huggingface_llm_image_uri(\u001b[39m\"\u001b[39m\u001b[39mhuggingface\u001b[39m\u001b[39m\"\u001b[39m,version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.8.2\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m \tenv\u001b[39m=\u001b[39mhub,\n\u001b[1;32m     22\u001b[0m \trole\u001b[39m=\u001b[39mrole, \n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[39m# deploy model to SageMaker Inference\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m predictor \u001b[39m=\u001b[39m huggingface_model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m     27\u001b[0m \tinitial_instance_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m \tinstance_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mml.g5.12xlarge\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     29\u001b[0m \tcontainer_startup_health_check_timeout\u001b[39m=\u001b[39;49m\u001b[39m400\u001b[39;49m,\n\u001b[1;32m     30\u001b[0m   )\n\u001b[1;32m     32\u001b[0m \u001b[39m# send request\u001b[39;00m\n\u001b[1;32m     33\u001b[0m predictor\u001b[39m.\u001b[39mpredict({\n\u001b[1;32m     34\u001b[0m \t\u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mMy name is Julien and I like to\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m })\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/sagemaker/huggingface/model.py:311\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39mand\u001b[39;00m instance_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m instance_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mml.inf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserving_image_uri(\n\u001b[1;32m    307\u001b[0m         region_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name,\n\u001b[1;32m    308\u001b[0m         instance_type\u001b[39m=\u001b[39minstance_type,\n\u001b[1;32m    309\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(HuggingFaceModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m    312\u001b[0m     initial_instance_count,\n\u001b[1;32m    313\u001b[0m     instance_type,\n\u001b[1;32m    314\u001b[0m     serializer,\n\u001b[1;32m    315\u001b[0m     deserializer,\n\u001b[1;32m    316\u001b[0m     accelerator_type,\n\u001b[1;32m    317\u001b[0m     endpoint_name,\n\u001b[1;32m    318\u001b[0m     tags,\n\u001b[1;32m    319\u001b[0m     kms_key,\n\u001b[1;32m    320\u001b[0m     wait,\n\u001b[1;32m    321\u001b[0m     data_capture_config,\n\u001b[1;32m    322\u001b[0m     async_inference_config,\n\u001b[1;32m    323\u001b[0m     serverless_inference_config,\n\u001b[1;32m    324\u001b[0m     volume_size\u001b[39m=\u001b[39;49mvolume_size,\n\u001b[1;32m    325\u001b[0m     model_data_download_timeout\u001b[39m=\u001b[39;49mmodel_data_download_timeout,\n\u001b[1;32m    326\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39;49mcontainer_startup_health_check_timeout,\n\u001b[1;32m    327\u001b[0m     inference_recommendation_id\u001b[39m=\u001b[39;49minference_recommendation_id,\n\u001b[1;32m    328\u001b[0m     explainer_config\u001b[39m=\u001b[39;49mexplainer_config,\n\u001b[1;32m    329\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/sagemaker/model.py:1328\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1326\u001b[0m     explainer_config_dict \u001b[39m=\u001b[39m explainer_config\u001b[39m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mendpoint_from_production_variants(\n\u001b[1;32m   1329\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_name,\n\u001b[1;32m   1330\u001b[0m     production_variants\u001b[39m=\u001b[39;49m[production_variant],\n\u001b[1;32m   1331\u001b[0m     tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m   1332\u001b[0m     kms_key\u001b[39m=\u001b[39;49mkms_key,\n\u001b[1;32m   1333\u001b[0m     wait\u001b[39m=\u001b[39;49mwait,\n\u001b[1;32m   1334\u001b[0m     data_capture_config_dict\u001b[39m=\u001b[39;49mdata_capture_config_dict,\n\u001b[1;32m   1335\u001b[0m     explainer_config_dict\u001b[39m=\u001b[39;49mexplainer_config_dict,\n\u001b[1;32m   1336\u001b[0m     async_inference_config_dict\u001b[39m=\u001b[39;49masync_inference_config_dict,\n\u001b[1;32m   1337\u001b[0m )\n\u001b[1;32m   1339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1340\u001b[0m     predictor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/sagemaker/session.py:4577\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4574\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating endpoint-config with name \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, name)\n\u001b[1;32m   4575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint_config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_options)\n\u001b[0;32m-> 4577\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_endpoint(endpoint_name\u001b[39m=\u001b[39;49mname, config_name\u001b[39m=\u001b[39;49mname, tags\u001b[39m=\u001b[39;49mtags, wait\u001b[39m=\u001b[39;49mwait)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/sagemaker/session.py:3970\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   3966\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   3967\u001b[0m     EndpointName\u001b[39m=\u001b[39mendpoint_name, EndpointConfigName\u001b[39m=\u001b[39mconfig_name, Tags\u001b[39m=\u001b[39mtags\n\u001b[1;32m   3968\u001b[0m )\n\u001b[1;32m   3969\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 3970\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_for_endpoint(endpoint_name)\n\u001b[1;32m   3971\u001b[0m \u001b[39mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/sagemaker/session.py:4322\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   4317\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   4318\u001b[0m             message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   4319\u001b[0m             allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   4320\u001b[0m             actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   4321\u001b[0m         )\n\u001b[0;32m-> 4322\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   4323\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   4324\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   4325\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   4326\u001b[0m     )\n\u001b[1;32m   4327\u001b[0m \u001b[39mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-06-14-12-16-29-069: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "\trole = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "\tiam = boto3.client('iam')\n",
    "\trole = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "\t'HF_MODEL_ID':'tiiuae/falcon-7b',\n",
    "\t'SM_NUM_GPUS': json.dumps(4)\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "\timage_uri=get_huggingface_llm_image_uri(\"huggingface\",version=\"0.8.2\"),\n",
    "\tenv=hub,\n",
    "\trole=role, \n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "\tinitial_instance_count=1,\n",
    "\tinstance_type=\"ml.g5.12xlarge\",\n",
    "\tcontainer_startup_health_check_timeout=400,\n",
    "  )\n",
    "  \n",
    "# send request\n",
    "predictor.predict({\n",
    "\t\"inputs\": \"My name is Julien and I like to\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::802376408542:role/Admin\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------!"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "# model config\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "max_input_length = 1024\n",
    "max_total_tokens = 2048\n",
    "health_check_timeout = 300\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "# retrieve the HuggingFace LLM DLC URI\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"huggingface-llm\",\n",
    "    region=sagemaker_session.boto_region_name,\n",
    "    version=\"0.8.2\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "# define environment variables for TGI config\n",
    "env = {\n",
    "    'HF_MODEL_ID': model_id,\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "    'MAX_INPUT_LENGTH': json.dumps(max_input_length),\n",
    "    'MAX_TOTEL_TOKENS': json.dumps(max_total_tokens),\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    role=aws_role,\n",
    "    env=env,\n",
    "    predictor_cls=Predictor,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"\\nHere's a Python program to compute Fibonacci sequence:\\n\\n```python\\ndef fibonacci(n):\\n    a, b = 0, 1\\n    for i in range(n-1):\\n        c = a + b\\n        a, b = b, c\\n    return c\\n\\nif __name__ == '__main__':\\n    print(fibonacci(10))\\n```\\n\\nThis program defines a function `fibonacci` that takes an integer argument `n` and returns the nth Fibonacci number. The program then calls the function with `n=10`, prints the result, and exits.\"}]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are an helpful Assistant, called Falcon. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Falcon:\"\"\"\n",
    "prompt = \"write a Python program to compute fibonacci sequence\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = predictor.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "assistant = response[0][\"generated_text\"][len(prompt):]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ExpiredTokenException) when calling the DescribeEndpoint operation: The security token included in the request is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m predictor\u001b[39m.\u001b[39;49mdelete_model()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m predictor\u001b[39m.\u001b[39mdelete_endpoint()\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/base_predictor.py:349\u001b[0m, in \u001b[0;36mPredictor.delete_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m request_failed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    348\u001b[0m failed_models \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 349\u001b[0m current_model_names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_model_names()\n\u001b[1;32m    350\u001b[0m \u001b[39mfor\u001b[39;00m model_name \u001b[39min\u001b[39;00m current_model_names:\n\u001b[1;32m    351\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/base_predictor.py:539\u001b[0m, in \u001b[0;36mPredictor._get_model_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    538\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_names\n\u001b[0;32m--> 539\u001b[0m current_endpoint_config_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_endpoint_config_name()\n\u001b[1;32m    540\u001b[0m endpoint_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mdescribe_endpoint_config(\n\u001b[1;32m    541\u001b[0m     EndpointConfigName\u001b[39m=\u001b[39mcurrent_endpoint_config_name\n\u001b[1;32m    542\u001b[0m )\n\u001b[1;32m    543\u001b[0m production_variants \u001b[39m=\u001b[39m endpoint_config[\u001b[39m\"\u001b[39m\u001b[39mProductionVariants\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/base_predictor.py:529\u001b[0m, in \u001b[0;36mPredictor._get_endpoint_config_name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint_config_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint_config_name\n\u001b[0;32m--> 529\u001b[0m endpoint_desc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mdescribe_endpoint(\n\u001b[1;32m    530\u001b[0m     EndpointName\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_name\n\u001b[1;32m    531\u001b[0m )\n\u001b[1;32m    532\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint_config_name \u001b[39m=\u001b[39m endpoint_desc[\u001b[39m\"\u001b[39m\u001b[39mEndpointConfigName\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    533\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_endpoint_config_name\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredTokenException) when calling the DescribeEndpoint operation: The security token included in the request is expired"
     ]
    }
   ],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from loguru import logger\n",
    "from pathlib import Path\n",
    "from safetensors.torch import save_file\n",
    "from safetensors import safe_open\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def check_file_size(source_file: Path, target_file: Path):\n",
    "    \"\"\"\n",
    "    Check that two files are close in size\n",
    "    \"\"\"\n",
    "    source_file_size = source_file.stat().st_size\n",
    "    target_file_size = target_file.stat().st_size\n",
    "\n",
    "    if (source_file_size - target_file_size) / source_file_size > 0.01:\n",
    "        raise RuntimeError(\n",
    "            f\"\"\"The file size different is more than 1%:\n",
    "         - {source_file}: {source_file_size}\n",
    "         - {target_file}: {target_file_size}\n",
    "         \"\"\"\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_shared_pointers(tensors: Dict[str, torch.Tensor]):\n",
    "    \"\"\"\n",
    "    For a Dict of tensors, check if two or more tensors point to the same underlying memory and\n",
    "    remove them\n",
    "    \"\"\"\n",
    "    ptrs = defaultdict(list)\n",
    "    for k, v in tensors.items():\n",
    "        ptrs[v.data_ptr()].append(k)\n",
    "\n",
    "    # Iterate over all found memory addresses\n",
    "    for ptr, names in ptrs.items():\n",
    "        if len(names) > 1:\n",
    "            # Multiple tensors are point to the same memory\n",
    "            # Only keep the first tensor\n",
    "            for name in names[1:]:\n",
    "                tensors.pop(name)\n",
    "\n",
    "\n",
    "def convert_file(pt_file: Path, sf_file: Path):\n",
    "    \"\"\"\n",
    "    Convert a pytorch file to a safetensors file\n",
    "    \"\"\"\n",
    "    logger.info(f\"Convert {pt_file} to {sf_file}.\")\n",
    "\n",
    "    pt_state = torch.load(pt_file, map_location=\"cpu\")\n",
    "    if \"state_dict\" in pt_state:\n",
    "        pt_state = pt_state[\"state_dict\"]\n",
    "\n",
    "    remove_shared_pointers(pt_state)\n",
    "\n",
    "    # Tensors need to be contiguous\n",
    "    pt_state = {k: v.contiguous() for k, v in pt_state.items()}\n",
    "\n",
    "    sf_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    save_file(pt_state, str(sf_file), metadata={\"format\": \"pt\"})\n",
    "\n",
    "    # Check that both files are close in size\n",
    "    check_file_size(pt_file, sf_file)\n",
    "\n",
    "    # Load safetensors state\n",
    "    for k in pt_state:\n",
    "        pt_tensor = pt_state[k]\n",
    "        with safe_open(sf_file, framework=\"pt\") as f:\n",
    "            sf_tensor = f.get_tensor(k)\n",
    "            if not torch.equal(pt_tensor, sf_tensor):\n",
    "                raise RuntimeError(f\"The output tensors do not match for key {k}\")\n",
    "\n",
    "\n",
    "def convert_files(pt_files: List[Path], sf_files: List[Path]):\n",
    "    assert len(pt_files) == len(sf_files)\n",
    "\n",
    "    N = len(pt_files)\n",
    "    # We do this instead of using tqdm because we want to parse the logs with the launcher\n",
    "\n",
    "    for i, (pt_file, sf_file) in enumerate(zip(pt_files, sf_files)):\n",
    "        start = datetime.datetime.now()\n",
    "        convert_file(pt_file, sf_file)\n",
    "        elapsed = datetime.datetime.now() - start\n",
    "        logger.info(f\"Convert: [{i + 1}/{N}] -- Took: {elapsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressing model snapshot directory ...\n",
      "./\n",
      "./tokenizer_config.json\n",
      "./README.md\n",
      "./handler.py\n",
      "./model-00008-of-00009.safetensors\n",
      "./tokenizer.json\n",
      "./model-00002-of-00009.safetensors\n",
      "./modelling_RW.py\n",
      "./model-00009-of-00009.safetensors\n",
      "./config.json\n",
      "./pytorch_model.bin.index.json\n",
      "./model-00005-of-00009.safetensors\n",
      "./model-00006-of-00009.safetensors\n",
      "./model-00007-of-00009.safetensors\n",
      "./.gitattributes\n",
      "./model-00004-of-00009.safetensors\n",
      "./model-00003-of-00009.safetensors\n",
      "./configuration_RW.py\n",
      "./special_tokens_map.json\n",
      "./generation_config.json\n",
      "./model-00001-of-00009.safetensors\n",
      "Uploading compressed model snapshot to S3 ...\n",
      "upload: models/falcon-40b-instruct.tar.gz to s3://sagemaker-jumpstart-cache-contributor-staging/jumpstart-1p/ulrichkr/falcon-40b-instruct.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['aws', 's3', 'cp', '/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/models/falcon-40b-instruct.tar.gz', 's3://sagemaker-jumpstart-cache-contributor-staging/jumpstart-1p/ulrichkr/falcon-40b-instruct.tar.gz'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "model_dir = Path.cwd() / \"models\" / model_id.split('/')[1]\n",
    "model_tarball_path = Path.cwd() / \"models\" / f\"{model_id.split('/')[1]}.tar.gz\"\n",
    "\n",
    "# print(\"Downloading snapshot of HuggingFace repository ...\")\n",
    "# snapshot_dir = snapshot_download(repo_id=model_id)\n",
    "\n",
    "# print(\"Copying snapshot to a model directory ...\")\n",
    "# copy_tree(snapshot_dir, str(model_dir))\n",
    "\n",
    "# local_pt_files =  list(model_dir.glob(\"*.bin\"))\n",
    "# local_st_files = [p.parent / f\"{p.stem.lstrip('pytorch_')}.safetensors\" for p in local_pt_files]\n",
    "# convert_files(local_pt_files, local_st_files)\n",
    "\n",
    "print(\"Compressing model snapshot directory ...\")\n",
    "command = [\n",
    "    'tar',\n",
    "    '--use-compress-program=\"pigz --best --recursive\"',\n",
    "    '-cvf',\n",
    "    str(model_tarball_path),\n",
    "    '-C',\n",
    "    str(model_dir),\n",
    "    '.'\n",
    "]\n",
    "subprocess.run(\" \".join(command), shell=True, check=True)\n",
    "\n",
    "print(\"Uploading compressed model snapshot to S3 ...\")\n",
    "model_s3_uri = f\"s3://sagemaker-jumpstart-cache-contributor-staging/jumpstart-1p/ulrichkr/{model_tarball_path.name}\"\n",
    "subprocess.run(f\"aws s3 cp {model_tarball_path} {model_s3_uri}\".split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------!"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "model_dir = Path.cwd() / \"models\" / model_id.split('/')[1]\n",
    "model_tarball_path = Path.cwd() / \"models\" / f\"{model_id.split('/')[1]}.tar.gz\"\n",
    "model_s3_uri = f\"s3://sagemaker-jumpstart-cache-contributor-staging/jumpstart-1p/ulrichkr/{model_tarball_path.name}\"\n",
    "\n",
    "# model config\n",
    "model_id = \"tiiuae/falcon-40b-instruct\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "max_input_length = 1024\n",
    "max_total_tokens = 2048\n",
    "health_check_timeout = 1200\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "# retrieve the HuggingFace LLM DLC URI\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"huggingface-llm\",\n",
    "    region=sagemaker_session.boto_region_name,\n",
    "    version=\"0.8.2\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "\n",
    "# define environment variables for TGI config\n",
    "env = {\n",
    "    'HF_MODEL_ID': \"/opt/ml/model\",\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu),\n",
    "    'MAX_INPUT_LENGTH': json.dumps(max_input_length),\n",
    "    'MAX_TOTEL_TOKENS': json.dumps(max_total_tokens),\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=model_s3_uri,\n",
    "    role=aws_role,\n",
    "    env=env,\n",
    "    predictor_cls=Predictor,\n",
    "    enable_network_isolation=True,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    "\n",
    "# TGI config\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"tiiuae/falcon-40b-instruct\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTEL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!"
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Amazon SageMaker is a fully managed platform for building, training, and deploying machine learning models at scale. It provides a range of tools and services to help data scientists and developers create and deploy ML models quickly and easily. These include pre-built algorithms, data processing tools, and integrated development environments. Additionally, SageMaker provides the ability to automate the end-to-end ML workflow, from data preparation to model training and deployment, making it easier to build and deploy ML models in production environments.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are an helpful Assistant, called Falcon. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Falcon:\"\"\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "assistant = response[0][\"generated_text\"][len(prompt):]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Amazon SageMaker provides several resources to help users get started with machine learning. Here are some suggestions:\n",
      "\n",
      "1. Start with the Getting Started tutorial on the SageMaker website, which guides you through the basics of creating a model.\n",
      "\n",
      "2. Use SageMaker's pre-built algorithms and tools, such as Amazon Rekognition, to get started with image and video analysis, natural language processing, and more.\n",
      "\n",
      "3. Join the AWS Machine Learning Community, where you can connect with other ML practitioners and get help from AWS experts.\n",
      "\n",
      "4. Consider taking an online course or attending a workshop to learn more about machine learning concepts and how to apply them using SageMaker.\n",
      "\n",
      "5. Finally, experiment with different models and algorithms to see what works best for your specific use case. SageMaker provides a range of tools and services to help you optimize and fine-tune your models over time.\n",
      "143\n",
      "13.836271420701376\n"
     ]
    }
   ],
   "source": [
    "new_prompt = f\"\"\"{prompt}{assistant}\n",
    "User: How would you recommend start using Amazon SageMaker? If i am new to Machine Learning?\n",
    "Falcon:\"\"\"\n",
    "# update payload\n",
    "payload[\"inputs\"] = new_prompt\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "new_assistant = response[0][\"generated_text\"][len(new_prompt):]\n",
    "print(new_assistant)\n",
    "print(len(new_assistant.split()))\n",
    "print(len(new_assistant.split()) / (time.time() - t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-tfhub-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
