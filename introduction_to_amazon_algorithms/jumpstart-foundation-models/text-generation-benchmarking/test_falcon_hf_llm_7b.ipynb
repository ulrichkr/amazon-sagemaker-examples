{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"sagemaker==2.163.0\" --upgrade --quiet --index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::802376408542:role/Admin\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n",
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=\"0.8.2\")\n",
    "print(llm_image)\n",
    "\n",
    "\n",
    "from sagemaker import Session\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "from sagemaker import image_uris\n",
    "# retrieve the HuggingFace LLM DLC URI\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"huggingface-llm\",\n",
    "    region=sagemaker_session.boto_region_name,\n",
    "    version=\"0.8.2\",\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "\n",
      "Error with deploying model: Role can not be null for deploying a model\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from concurrent import futures\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from sagemaker.session import Session\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "\n",
    "\n",
    "NUM_DEPLOYMENTS = 30\n",
    "MAX_CONCURRENT_DEPLOYMENTS = 10\n",
    "SM_SESSION = Session(\n",
    "    sagemaker_client=boto3.client(\n",
    "        \"sagemaker\",\n",
    "        config=Config(connect_timeout=5, read_timeout=60, retries={\"max_attempts\": 20}),\n",
    "    )\n",
    ")\n",
    "SM_ROLE = sagemaker.get_execution_role()\n",
    "\n",
    "def deploy_llm_endpoint(\n",
    "        hf_model_id: str,\n",
    "        instance_type: str = \"ml.g5.48xlarge\",\n",
    "        number_of_gpu: int = 4,\n",
    "        max_input_length: int = 1024,\n",
    "        max_total_tokens: int = 2048,\n",
    "        health_check_timeout: int = 600\n",
    ") -> bool:\n",
    "    success = False\n",
    "    try:\n",
    "        llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=\"0.8.2\")\n",
    "        env = {\n",
    "            \"HF_MODEL_ID\": hf_model_id,\n",
    "            \"SM_NUM_GPUS\": str(number_of_gpu),\n",
    "            \"MAX_INPUT_LENGTH\": str(max_input_length),\n",
    "            \"MAX_TOTAL_TOKENS\": str(max_total_tokens),\n",
    "        }\n",
    "        model = HuggingFaceModel(role=SM_ROLE, image_uri=llm_image, env=env, sagemaker_session=SM_SESSION)\n",
    "        predictor = model.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=instance_type,\n",
    "            container_startup_health_check_timeout=health_check_timeout,\n",
    "        )\n",
    "        predictor.delete_model()\n",
    "        predictor.delete_endpoint()\n",
    "        success = True\n",
    "        print(\"\\nSuccessful deployment.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError with deploying model: {e}\")\n",
    "    return success\n",
    "\n",
    "\n",
    "with futures.ThreadPoolExecutor(max_workers=NUM_DEPLOYMENTS) as executor:\n",
    "    results = executor.map(deploy_llm_endpoint, [\"tiiuae/falcon-40b-instruct\"] * NUM_DEPLOYMENTS)\n",
    "\n",
    "results = list(results)\n",
    "print(results)\n",
    "print(sum(results) / len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"0.8.2\"\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "number_of_gpu = 4\n",
    "health_check_timeout = 300\n",
    "\n",
    "# TGI config\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"tiiuae/falcon-7b-instruct\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n",
    "  'MAX_TOTEL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n",
    "  # 'HF_MODEL_QUANTIZE': \"bitsandbytes\", # comment in to quantize\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint huggingface-pytorch-tgi-inference-2023-06-08-21-53-38-431: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm_7b.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm_7b.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m llm_model\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm_7b.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m   initial_instance_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm_7b.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m   instance_type\u001b[39m=\u001b[39;49minstance_type,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm_7b.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m   \u001b[39m# volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm_7b.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m   container_startup_health_check_timeout\u001b[39m=\u001b[39;49mhealth_check_timeout, \u001b[39m# 10 minutes to be able to load the model\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/test_falcon_hf_llm_7b.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/huggingface/model.py:311\u001b[0m, in \u001b[0;36mHuggingFaceModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39mand\u001b[39;00m instance_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m instance_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mml.inf\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    306\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_uri \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mserving_image_uri(\n\u001b[1;32m    307\u001b[0m         region_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mboto_session\u001b[39m.\u001b[39mregion_name,\n\u001b[1;32m    308\u001b[0m         instance_type\u001b[39m=\u001b[39minstance_type,\n\u001b[1;32m    309\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(HuggingFaceModel, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mdeploy(\n\u001b[1;32m    312\u001b[0m     initial_instance_count,\n\u001b[1;32m    313\u001b[0m     instance_type,\n\u001b[1;32m    314\u001b[0m     serializer,\n\u001b[1;32m    315\u001b[0m     deserializer,\n\u001b[1;32m    316\u001b[0m     accelerator_type,\n\u001b[1;32m    317\u001b[0m     endpoint_name,\n\u001b[1;32m    318\u001b[0m     tags,\n\u001b[1;32m    319\u001b[0m     kms_key,\n\u001b[1;32m    320\u001b[0m     wait,\n\u001b[1;32m    321\u001b[0m     data_capture_config,\n\u001b[1;32m    322\u001b[0m     async_inference_config,\n\u001b[1;32m    323\u001b[0m     serverless_inference_config,\n\u001b[1;32m    324\u001b[0m     volume_size\u001b[39m=\u001b[39;49mvolume_size,\n\u001b[1;32m    325\u001b[0m     model_data_download_timeout\u001b[39m=\u001b[39;49mmodel_data_download_timeout,\n\u001b[1;32m    326\u001b[0m     container_startup_health_check_timeout\u001b[39m=\u001b[39;49mcontainer_startup_health_check_timeout,\n\u001b[1;32m    327\u001b[0m     inference_recommendation_id\u001b[39m=\u001b[39;49minference_recommendation_id,\n\u001b[1;32m    328\u001b[0m     explainer_config\u001b[39m=\u001b[39;49mexplainer_config,\n\u001b[1;32m    329\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/model.py:1328\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[39mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1326\u001b[0m     explainer_config_dict \u001b[39m=\u001b[39m explainer_config\u001b[39m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mendpoint_from_production_variants(\n\u001b[1;32m   1329\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint_name,\n\u001b[1;32m   1330\u001b[0m     production_variants\u001b[39m=\u001b[39;49m[production_variant],\n\u001b[1;32m   1331\u001b[0m     tags\u001b[39m=\u001b[39;49mtags,\n\u001b[1;32m   1332\u001b[0m     kms_key\u001b[39m=\u001b[39;49mkms_key,\n\u001b[1;32m   1333\u001b[0m     wait\u001b[39m=\u001b[39;49mwait,\n\u001b[1;32m   1334\u001b[0m     data_capture_config_dict\u001b[39m=\u001b[39;49mdata_capture_config_dict,\n\u001b[1;32m   1335\u001b[0m     explainer_config_dict\u001b[39m=\u001b[39;49mexplainer_config_dict,\n\u001b[1;32m   1336\u001b[0m     async_inference_config_dict\u001b[39m=\u001b[39;49masync_inference_config_dict,\n\u001b[1;32m   1337\u001b[0m )\n\u001b[1;32m   1339\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1340\u001b[0m     predictor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor_cls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/session.py:4577\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict)\u001b[0m\n\u001b[1;32m   4574\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating endpoint-config with name \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, name)\n\u001b[1;32m   4575\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint_config(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig_options)\n\u001b[0;32m-> 4577\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_endpoint(endpoint_name\u001b[39m=\u001b[39;49mname, config_name\u001b[39m=\u001b[39;49mname, tags\u001b[39m=\u001b[39;49mtags, wait\u001b[39m=\u001b[39;49mwait)\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/session.py:3970\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   3966\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_client\u001b[39m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   3967\u001b[0m     EndpointName\u001b[39m=\u001b[39mendpoint_name, EndpointConfigName\u001b[39m=\u001b[39mconfig_name, Tags\u001b[39m=\u001b[39mtags\n\u001b[1;32m   3968\u001b[0m )\n\u001b[1;32m   3969\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 3970\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_for_endpoint(endpoint_name)\n\u001b[1;32m   3971\u001b[0m \u001b[39mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/sagemaker/session.py:4322\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   4316\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   4317\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   4318\u001b[0m             message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   4319\u001b[0m             allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   4320\u001b[0m             actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   4321\u001b[0m         )\n\u001b[0;32m-> 4322\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   4323\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   4324\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mInService\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   4325\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   4326\u001b[0m     )\n\u001b[1;32m   4327\u001b[0m \u001b[39mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint huggingface-pytorch-tgi-inference-2023-06-08-21-53-38-431: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Amazon SageMaker is a fully managed platform for building, training, and deploying machine learning models at scale. It provides a range of tools and services to help data scientists and developers create and deploy ML models quickly and easily. These include pre-built algorithms, data processing tools, and integrated development environments. Additionally, SageMaker provides the ability to automate the end-to-end ML workflow, from data preparation to model training and deployment, making it easier to build and deploy ML models in production environments.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are an helpful Assistant, called Falcon. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Falcon:\"\"\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.8,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"repetition_penalty\": 1.03,\n",
    "    \"stop\": [\"\\nUser:\",\"<|endoftext|>\",\"</s>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "assistant = response[0][\"generated_text\"][len(prompt):]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " If you are new to machine learning, you may want to start with the SageMaker JumpStart program, which provides hands-on experience with the platform and a range of ML algorithms. You can also explore the SageMaker Quick Start guides, which provide step-by-step instructions for building and deploying ML models using popular frameworks such as TensorFlow and PyTorch. Additionally, you may want to consider taking online courses or attending workshops to learn more about machine learning and how to use SageMaker. Finally, don't be afraid to experiment with different algorithms and approaches to find the best solution for your specific problem.\n",
      "100\n",
      "14.789933982576365\n"
     ]
    }
   ],
   "source": [
    "new_prompt = f\"\"\"{prompt}{assistant}\n",
    "User: How would you recommend start using Amazon SageMaker? If i am new to Machine Learning?\n",
    "Falcon:\"\"\"\n",
    "# update payload\n",
    "payload[\"inputs\"] = new_prompt\n",
    "\n",
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "# send request to endpoint\n",
    "response = llm.predict(payload)\n",
    "\n",
    "# print assistant respond\n",
    "new_assistant = response[0][\"generated_text\"][len(new_prompt):]\n",
    "print(new_assistant)\n",
    "print(len(new_assistant.split()))\n",
    "print(len(new_assistant.split()) / (time.time() - t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-tfhub-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
