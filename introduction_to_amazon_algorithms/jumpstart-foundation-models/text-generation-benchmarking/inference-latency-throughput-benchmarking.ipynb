{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e48755a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker JumpStart Foundation Models - Inference Latency and Throughput Benchmarking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59b5a329",
   "metadata": {},
   "source": [
    "***\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use SageMaker JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "\n",
    "In this demo notebook, we demonstrate how to run latency and throughput benchmarking analyses on a set of SageMaker JumpStart models. The structure of the notebook allows you to both benchmark a single model against multiple payloads and multiple models against a single payload. \n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46c6f117",
   "metadata": {},
   "source": [
    "1. [Set up](#1.-Set-up)\n",
    "2. [Run latency and throughput benchmarking](#2.-Run-latency-and-throughput-benchmarking)\n",
    "3. [Visualize benchmarking results](#3.-Visualize-benchmarking-results)\n",
    "4. [Clean up](#4.-Clean-up)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c59ee575",
   "metadata": {},
   "source": [
    "### 1. Set up\n",
    "\n",
    "***\n",
    "Before executing the notebook, there are some initial steps required for set up. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f43e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://amazon-149122183214.d.codeartifact.us-west-2.amazonaws.com/pypi/sagemaker-studio-shared/simple/sagemaker/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: 401 Error, Credentials not correct for https://amazon-149122183214.d.codeartifact.us-west-2.amazonaws.com/pypi/sagemaker-studio-shared/simple/ipywidgets/\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sagemaker ipywidgets --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e2bd729",
   "metadata": {},
   "source": [
    "***\n",
    "Here, you will query the SageMaker SDK to return a list of all HuggingFace text generation (and text2text) models hosted by SageMaker Model Hub. You can manually select any combination of these models to run benchmarking on with the Jupyter Widget produced in the output of this cell. By default, only a few models are selected.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd6eceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e361e0fb08a4289bcb18c495c9fbc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SelectMultiple(description='Models:', index=(27, 26, 29, 28), layout=Layout(width='100%'), options=('huggingfaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import SelectMultiple, Layout\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from sagemaker.jumpstart.filters import And, Or\n",
    "\n",
    "# Retrieves all Text Generation models available by SageMaker Built-In Algorithms.\n",
    "tasks = [\"textgeneration\", \"textgeneration1\", \"textgeneration2\", \"text2text\"]\n",
    "filter_value = And(Or(*[f\"task == {task}\" for task in tasks]), \"framework == huggingface\")\n",
    "text_models = list_jumpstart_models(filter=filter_value)\n",
    "selected_text_models = [\n",
    "    \"huggingface-textgeneration-falcon-40b-instruct-bf16\",\n",
    "    \"huggingface-textgeneration-falcon-40b-bf16\",\n",
    "    \"huggingface-textgeneration-falcon-7b-instruct-bf16\",\n",
    "    \"huggingface-textgeneration-falcon-7b-bf16\",\n",
    "    # \"huggingface-text2text-flan-t5-xxl\",\n",
    "    # \"huggingface-textgeneration1-gpt-j-6b\",\n",
    "    # \"huggingface-textgeneration2-gpt-neoxt-chat-base-20b-fp16\",\n",
    "    # \"huggingface-textgeneration-bloom-1b7\",\n",
    "]\n",
    "# if you would like to run on all JumpStart LLMs instead, uncomment the following line.\n",
    "# selected_text_models = text_models.copy()\n",
    "# selected_text_models.remove(\"huggingface-textgeneration1-bloom-176b-int8\")\n",
    "# selected_text_models.remove(\"huggingface-textgeneration1-bloomz-176b-fp16\")\n",
    "\n",
    "models_selection = SelectMultiple(\n",
    "    options=text_models,\n",
    "    value=selected_text_models,\n",
    "    description=\"Models:\",\n",
    "    rows=25,\n",
    "    layout=Layout(width=\"100%\"),\n",
    ")\n",
    "display(models_selection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60d8e537",
   "metadata": {},
   "source": [
    "***\n",
    "In the following cell, you will select the models and payloads to benchmark. Every payload will be benchmarked against every model.\n",
    "- **MODELS**: A list of SageMaker JumpStart model IDs to run benchmarking against.\n",
    "- **PAYLOADS**: A dictionary with keys identifying a unique name for a query payload and values containing a valid payload dictionary.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcc999de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = models_selection.value\n",
    "\n",
    "PAYLOADS = {\n",
    "    \"simple_short_input\": {\n",
    "        \"text_inputs\": \"Hello!\",\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 25,\n",
    "    },\n",
    "    \"generate_summary\": {\n",
    "        \"text_inputs\": (\n",
    "            \"Write a short summary for this text: Amazon Comprehend uses natural language \"\n",
    "            \"processing (NLP) to extract insights about the content of documents. It develops \"\n",
    "            \"insights by recognizing the entities, key phrases, language, sentiments, and other \"\n",
    "            \"common elements in a document. Use Amazon Comprehend to create new products based on \"\n",
    "            \"understanding the structure of documents. For example, using Amazon Comprehend you \"\n",
    "            \"can search social networking feeds for mentions of products or scan an entire \"\n",
    "            \"document repository for key phrases. \\nYou can access Amazon Comprehend document \"\n",
    "            \"analysis capabilities using the Amazon Comprehend console or using the Amazon \"\n",
    "            \"Comprehend APIs. You can run real-time analysis for small workloads or you can start \"\n",
    "            \"asynchronous analysis jobs for large document sets. You can use the pre-trained \"\n",
    "            \"models that Amazon Comprehend provides, or you can train your own custom models for \"\n",
    "            \"classification and entity recognition. \\nAll of the Amazon Comprehend features \"\n",
    "            \"accept UTF-8 text documents as the input. In addition, custom classification and \"\n",
    "            \"custom entity recognition accept image files, PDF files, and Word files as input. \\n\"\n",
    "            \"Amazon Comprehend can examine and analyze documents in a variety of languages, \"\n",
    "            \"depending on the specific feature. For more information, see Languages supported in \"\n",
    "            \"Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine \"\n",
    "            \"documents and determine the dominant language for a far wider selection of languages.\"\n",
    "        ),\n",
    "        \"do_sample\": True,\n",
    "        # \"max_length\": 500,\n",
    "        \"max_new_tokens\": 25,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22f60ab8",
   "metadata": {},
   "source": [
    "***\n",
    "The following set of constants drive the behavior of this notebook:\n",
    "- **MAX_CONCURRENT_INVOCATIONS_PER_MODEL**: The maximum number of endpoint predictions to request concurrently.\n",
    "- **MAX_CONCURRENT_BENCHMARKS**: The maximum number of models to concurrently benchmark.\n",
    "- **RETRY_WAIT_TIME_SECONDS**: The amount of time in seconds to wait between Amazon CloudWatch queries. This is necessary because the endpoint emits CloudWatch metrics on a periodic interval, so we need to wait until all samples are emitted to CloudWatch before publishing benchmarking statistics.\n",
    "- **MAX_TOTAL_RETRY_TIME_SECONDS**: The maximum amount of time in seconds to wait on Amazon CloudWatch emissions before proceeding without collecting the requested benchmarking metrics.\n",
    "- **NUM_INVOCATIONS**: The number of endpoint predictions to request per benchmark.\n",
    "- **SAVE_METRICS_FILE_PATH**: The JSON file used to save the resulting metrics.\n",
    "- **SM_SESSION**: SageMaker Session object with custom configuration to resolve [SDK rate exceeded and throttling exceptions](https://aws.amazon.com/premiumsupport/knowledge-center/sagemaker-python-throttlingexception/).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccb86ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "MAX_CONCURRENT_BENCHMARKS = 50\n",
    "NUM_INVOCATIONS = 10\n",
    "SAVE_METRICS_FILE_PATH = Path.cwd() / \"latency_benchmarking.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd6047aa",
   "metadata": {},
   "source": [
    "### 2. Run latency and throughput benchmarking\n",
    "\n",
    "***\n",
    "\n",
    "The following block defines a function to run benchmarking on a single SageMaker JumpStart model ID. This function performs the following actions:\n",
    "- Create a SageMaker JumpStart `Model` object.\n",
    "- Deploy the Model and obtain a `Predictor`.\n",
    "- Run all benchmarking load tests for each payload defined in the `PAYLOADS` dictionary. The benchmarking process includes:\n",
    "  - Obtain latency statistics - serially invoke an endpoint to obtain a batch of predictions and utilize the Amazon CloudWatch [GetMetricStatistics](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_GetMetricStatistics.html) API to obtain latency statistics regarding the batch of predictions. The endpoint is invoked `NUM_INVOCATIONS` times.\n",
    "  - Obtain throughput statistics - concurrently invoke an endpoint to obtain client-side throughput statistics. The endpoint is invoked `NUM_INVOCATIONS` times.\n",
    "- Clean up predictor model and endpoint. If any errors occur during the benchmarking process for a given model, this clean up process still occurs prior to raising the error.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1b3bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17f3350",
   "metadata": {},
   "source": [
    "***\n",
    "In the following block, the `run_benchmarking` function is called for all model IDs specified within the previously defined `MODELS` list. To avoid a serial deployment process, the Python standard library [concurrent futures](https://docs.python.org/3/library/concurrent.futures.html) module is used to concurrently execute a `MAX_CONCURRENT_BENCHMARKS` number of executor threads. When a thread completes execution, the computed metrics are extended into a single list. If any thread raises an error instead of returning metrics, the errors are recorded in a dictionary without re-raising the error. This allows benchmarking to continue for all other models.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7460a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Model 'huggingface-textgeneration-falcon-40b-instruct-bf16'): Deploying endpoint jumpstart-bm-hf-textgeneration-falcon-4-2023-06-09-04-52-32-299 ...\n",
      "(Model 'huggingface-textgeneration-falcon-7b-bf16'): Deploying endpoint jumpstart-bm-hf-textgeneration-falcon-7-2023-06-09-04-52-32-372 ...\n",
      "(Model 'huggingface-textgeneration-falcon-40b-bf16'): Deploying endpoint jumpstart-bm-hf-textgeneration-falcon-4-2023-06-09-04-52-32-379 ...\n",
      "(Model 'huggingface-textgeneration-falcon-7b-instruct-bf16'): Deploying endpoint jumpstart-bm-hf-textgeneration-falcon-7-2023-06-09-04-52-32-385 ...\n",
      "-----------------------------------------------!(Model huggingface-textgeneration-falcon-7b-bf16) Benchmarking failed: run_single_predictor() missing 1 required positional argument: 'payloads'\n",
      "-----------------!(Model huggingface-textgeneration-falcon-7b-instruct-bf16) Benchmarking failed: run_single_predictor() missing 1 required positional argument: 'payloads'\n",
      "--------------------------!(Model huggingface-textgeneration-falcon-40b-bf16) Benchmarking failed: run_single_predictor() missing 1 required positional argument: 'payloads'\n",
      "--!(Model huggingface-textgeneration-falcon-40b-instruct-bf16) Benchmarking failed: run_single_predictor() missing 1 required positional argument: 'payloads'\n"
     ]
    }
   ],
   "source": [
    "from benchmarking.runner import Benchmarker\n",
    "\n",
    "\n",
    "benchmarker = Benchmarker(payloads=PAYLOADS, max_concurrent_benchmarks=MAX_CONCURRENT_BENCHMARKS)\n",
    "metrics, errors = benchmarker.run_multiple_model_ids(models=MODELS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c3470dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "Finally, we save these benchmarked metrics to a JSON file for use in downstream analyses.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d743be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "output = {\"models\": MODELS, \"payloads\": PAYLOADS, \"metrics\": metrics}\n",
    "with open(SAVE_METRICS_FILE_PATH, \"w\") as file:\n",
    "    json.dump(output, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a484d018",
   "metadata": {},
   "source": [
    "### 3. Visualize benchmarking results\n",
    "\n",
    "***\n",
    "The saved JSON results are now re-loaded into a normalized pandas DataFrame for visualization. This cell shows the following:\n",
    "1. The column names of the DataFrame. These are the available statistics you are able to explore.\n",
    "2. A table that shows a sample output from each model ID in `MODELS` for each payload in `PAYLOAD`.\n",
    "3. A table that shows key latency and throughput statistics for each model ID in `MODELS` and each payload in `PAYLOAD`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50763cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the available statistics:  []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['PayloadName', 'ModelID', 'SampleOutput'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb Cell 19\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m display_cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPayloadName\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mModelID\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSampleOutput\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m sort_cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPayloadName\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m display(df[display_cols]\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39msort_cols)\u001b[39m.\u001b[39mset_index(index_cols))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m display_cols \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPayloadName\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mModelID\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mClient.LatencyPerOutputWord.Average\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-13-58-252-142.us-east-2.compute.amazonaws.com/home/ubuntu/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/inference-latency-throughput-benchmarking.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m sort_cols \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mPayloadName\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mClient.LatencyPerOutputWord.Average\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/pandas/core/frame.py:3766\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3764\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3765\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3766\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3768\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3769\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/pandas/core/indexes/base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5873\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5876\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5878\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5879\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5880\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/test-tfhub-py39/lib/python3.9/site-packages/pandas/core/indexes/base.py:5935\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5933\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5934\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 5935\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5937\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   5938\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['PayloadName', 'ModelID', 'SampleOutput'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "df = pd.json_normalize(metrics)\n",
    "print(\"Here are the available statistics: \", list(df.columns))\n",
    "\n",
    "index_cols = [\"PayloadName\", \"ModelID\"]\n",
    "display_cols = [\"PayloadName\", \"ModelID\", \"SampleOutput\"]\n",
    "sort_cols = [\"PayloadName\"]\n",
    "display(df[display_cols].sort_values(by=sort_cols).set_index(index_cols))\n",
    "\n",
    "display_cols = [\n",
    "    \"PayloadName\",\n",
    "    \"ModelID\",\n",
    "    \"Throughput\",\n",
    "    \"ModelLatency.Average\",\n",
    "    \"Client.Latency.Average\",\n",
    "    \"Client.OutputSequenceWords.Average\",\n",
    "    \"WordThroughput\",\n",
    "    \"Client.LatencyPerOutputWord.Average\",\n",
    "]\n",
    "sort_cols = [\"PayloadName\", \"Client.LatencyPerOutputWord.Average\"]\n",
    "display(df[display_cols].sort_values(by=sort_cols).set_index(index_cols).round(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d6caad2",
   "metadata": {},
   "source": [
    "***\n",
    "Finally, we show some plots based on this latency analysis. For each payload, this cell creates a plotly figure that plots the average latency per output word versus word throughput, or the number of words in output sequences returned per second by the model. In general, throughput = 1 / latency. However, multi-model endpoints and load-balanced endpoints can improve throughput for a fixed latency. Both of these are important metrics to consider when designing requirements for model selection.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0cd06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "for payload_name in PAYLOADS:\n",
    "    col_x, col_y = \"WordThroughput\", \"Client.LatencyPerOutputWord.Average\"\n",
    "    df_plot = df[df[\"PayloadName\"] == payload_name]\n",
    "    fig = px.scatter(df_plot, x=col_x, y=col_y, hover_data=[\"ModelID\"])\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.linspace(1, 300, 300), y=1 / np.linspace(1, 300, 300), name=\"y=1/x\")\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_range=[0.0, df_plot[col_x].max() * 1.1],\n",
    "        yaxis_range=[0.0, df_plot[col_y].max() * 1.1],\n",
    "        title=f\"Latency per word vs. word throughput for payload {payload_name}\",\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50f34b58",
   "metadata": {},
   "source": [
    "### 4. Clean up\n",
    "\n",
    "***\n",
    "When you are done with the endpoints, you should delete them to avoid additional costs. In this demonstration, clean up occurs at the end of each individual benchmarking job.\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-tfhub-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
