{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "\n",
    "os.environ[\"AWS_JUMPSTART_MANIFEST_LOCAL_ROOT_DIR_OVERRIDE\"] = \\\n",
    "    \"/home/ubuntu/code/aws/sagemaker-model-hub/src/sagemakermodelhub/content_generation/metadata/generated_metadata/sdk/manifests/\"\n",
    "os.environ[\"AWS_JUMPSTART_SPECS_LOCAL_ROOT_DIR_OVERRIDE\"] = \\\n",
    "    \"/home/ubuntu/code/aws/sagemaker-model-hub/src/sagemakermodelhub/content_generation/metadata/generated_metadata/sdk/specs/\"\n",
    "model_uri = \"s3://sagemaker-jumpstart-cache-contributor-staging/jumpstart-1p/generated/huggingface/llm/huggingface-llm-open-llama-13b-fp16/inference/infer-huggingface-llm-open-llama-13b-fp16-20230620-2119.tar.gz\"\n",
    "\n",
    "\n",
    "model_id = \"huggingface-llm-open-llama-13b-fp16\"\n",
    "model = JumpStartModel(\n",
    "    model_id=model_id,\n",
    "    env={\n",
    "        # \"SM_NUM_GPUS\": \"8\",\n",
    "        'MAX_INPUT_LENGTH': \"20000\",\n",
    "        'MAX_TOTAL_TOKENS': \"30000\", \n",
    "    },\n",
    "    model_data=model_uri\n",
    "    # instance_type=\"ml.g5.48xlarge\"\n",
    ")\n",
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=\"hf-llm-falcon-40b-instruct-bf16-2023-06-27-12-53-49-511\",\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running probe on max_new_tokens, starting with {'input_length': 100, 'max_new_tokens': 500, 'concurrent_requests': 1} ...\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 15.47s, iteration 0\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 600, 'concurrent_requests': 1}, duration 18.49s, iteration 1\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 720, 'concurrent_requests': 1}, duration 22.32s, iteration 2\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 864, 'concurrent_requests': 1}, duration 27.00s, iteration 3\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 1037, 'concurrent_requests': 1}, duration 32.48s, iteration 4\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 1245, 'concurrent_requests': 1}, duration 39.26s, iteration 5\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 1494, 'concurrent_requests': 1}, duration 47.29s, iteration 6\n",
      " - Success with {'input_length': 100, 'max_new_tokens': 1793, 'concurrent_requests': 1}, duration 56.85s, iteration 7\n",
      " - Failed with {'input_length': 100, 'max_new_tokens': 2152, 'concurrent_requests': 1} with error An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-27-12-53-49-511 in account 802376408542 for more information..\n",
      " - Waiting for 3 minutes to allow endpoint time to recover.\n",
      "Running probe on max_new_tokens, starting with {'input_length': 1000, 'max_new_tokens': 500, 'concurrent_requests': 1} ...\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 16.88s, iteration 0\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 600, 'concurrent_requests': 1}, duration 19.87s, iteration 1\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 720, 'concurrent_requests': 1}, duration 23.62s, iteration 2\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 864, 'concurrent_requests': 1}, duration 28.26s, iteration 3\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 1037, 'concurrent_requests': 1}, duration 33.82s, iteration 4\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 1245, 'concurrent_requests': 1}, duration 40.61s, iteration 5\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 1494, 'concurrent_requests': 1}, duration 48.68s, iteration 6\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 1793, 'concurrent_requests': 1}, duration 58.72s, iteration 7\n",
      " - Failed with {'input_length': 1000, 'max_new_tokens': 2152, 'concurrent_requests': 1} with error An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-27-12-53-49-511 in account 802376408542 for more information..\n",
      " - Waiting for 3 minutes to allow endpoint time to recover.\n",
      "Running probe on input_length, starting with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 1} ...\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 3.81s, iteration 0\n",
      " - Success with {'input_length': 600, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 3.72s, iteration 1\n",
      " - Success with {'input_length': 720, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 3.88s, iteration 2\n",
      " - Success with {'input_length': 864, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 4.02s, iteration 3\n",
      " - Success with {'input_length': 1037, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 4.26s, iteration 4\n",
      " - Success with {'input_length': 1245, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 4.43s, iteration 5\n",
      " - Success with {'input_length': 1494, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 4.71s, iteration 6\n",
      " - Success with {'input_length': 1793, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 5.07s, iteration 7\n",
      " - Success with {'input_length': 2152, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 5.50s, iteration 8\n",
      " - Success with {'input_length': 2583, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 6.04s, iteration 9\n",
      " - Success with {'input_length': 3100, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 6.63s, iteration 10\n",
      " - Success with {'input_length': 3720, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 7.32s, iteration 11\n",
      " - Success with {'input_length': 4464, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 8.24s, iteration 12\n",
      " - Success with {'input_length': 5357, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 9.32s, iteration 13\n",
      " - Success with {'input_length': 6429, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 10.60s, iteration 14\n",
      " - Success with {'input_length': 7715, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 12.44s, iteration 15\n",
      " - Success with {'input_length': 9258, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 15.34s, iteration 16\n",
      " - Success with {'input_length': 11110, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 17.50s, iteration 17\n",
      " - Success with {'input_length': 13332, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 20.27s, iteration 18\n",
      " - Success with {'input_length': 15999, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 24.92s, iteration 19\n",
      " - Success with {'input_length': 19199, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 29.14s, iteration 20\n",
      " - Failed with {'input_length': 23039, 'max_new_tokens': 100, 'concurrent_requests': 1} with error An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (422) from primary with message \"{\"error\":\"Input validation error: `inputs` must have less than 20000 tokens. Given: 23040\",\"error_type\":\"validation\"}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-27-12-53-49-511 in account 802376408542 for more information..\n",
      " - Waiting for 3 minutes to allow endpoint time to recover.\n",
      "Running probe on input_length, starting with {'input_length': 500, 'max_new_tokens': 500, 'concurrent_requests': 1} ...\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 16.22s, iteration 0\n",
      " - Success with {'input_length': 600, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 16.23s, iteration 1\n",
      " - Success with {'input_length': 720, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 16.38s, iteration 2\n",
      " - Success with {'input_length': 864, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 16.43s, iteration 3\n",
      " - Success with {'input_length': 1037, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 16.70s, iteration 4\n",
      " - Success with {'input_length': 1245, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 17.03s, iteration 5\n",
      " - Success with {'input_length': 1494, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 17.41s, iteration 6\n",
      " - Success with {'input_length': 1793, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 17.83s, iteration 7\n",
      " - Success with {'input_length': 2152, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 18.49s, iteration 8\n",
      " - Success with {'input_length': 2583, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 19.18s, iteration 9\n",
      " - Success with {'input_length': 3100, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 19.98s, iteration 10\n",
      " - Success with {'input_length': 3720, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 20.94s, iteration 11\n",
      " - Success with {'input_length': 4464, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 22.23s, iteration 12\n",
      " - Success with {'input_length': 5357, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 23.86s, iteration 13\n",
      " - Success with {'input_length': 6429, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 25.83s, iteration 14\n",
      " - Success with {'input_length': 7715, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 27.89s, iteration 15\n",
      " - Success with {'input_length': 9258, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 31.30s, iteration 16\n",
      " - Success with {'input_length': 11110, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 34.40s, iteration 17\n",
      " - Success with {'input_length': 13332, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 36.69s, iteration 18\n",
      " - Success with {'input_length': 15999, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 43.88s, iteration 19\n",
      " - Success with {'input_length': 19199, 'max_new_tokens': 500, 'concurrent_requests': 1}, duration 49.30s, iteration 20\n",
      " - Failed with {'input_length': 23039, 'max_new_tokens': 500, 'concurrent_requests': 1} with error An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (422) from primary with message \"{\"error\":\"Input validation error: `inputs` must have less than 20000 tokens. Given: 23040\",\"error_type\":\"validation\"}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-27-12-53-49-511 in account 802376408542 for more information..\n",
      " - Waiting for 3 minutes to allow endpoint time to recover.\n",
      "Running probe on concurrent_requests, starting with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 1} ...\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 3.83s, iteration 0\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 2}, duration 4.31s, iteration 1\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 3}, duration 4.98s, iteration 2\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 4}, duration 5.60s, iteration 3\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 5}, duration 6.24s, iteration 4\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 6}, duration 6.90s, iteration 5\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 8}, duration 8.16s, iteration 6\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 10}, duration 9.56s, iteration 7\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 12}, duration 10.82s, iteration 8\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 15}, duration 12.95s, iteration 9\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 18}, duration 15.63s, iteration 10\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 22}, duration 18.55s, iteration 11\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 27}, duration 22.28s, iteration 12\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 33}, duration 27.14s, iteration 13\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 40}, duration 32.40s, iteration 14\n",
      " - Success with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 48}, duration 37.81s, iteration 15\n",
      " - Failed with {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 58} with error An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-27-12-53-49-511 in account 802376408542 for more information..\n",
      " - Waiting for 3 minutes to allow endpoint time to recover.\n",
      "Running probe on concurrent_requests, starting with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 1} ...\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 1}, duration 5.37s, iteration 0\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 2}, duration 5.46s, iteration 1\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 3}, duration 6.59s, iteration 2\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 4}, duration 7.85s, iteration 3\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 5}, duration 8.96s, iteration 4\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 6}, duration 11.41s, iteration 5\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 8}, duration 12.82s, iteration 6\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 10}, duration 16.06s, iteration 7\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 12}, duration 18.27s, iteration 8\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 15}, duration 22.16s, iteration 9\n",
      " - Success with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 18}, duration 26.58s, iteration 10\n",
      " - Failed with {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 22} with error An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\"error\":\"Request failed during generation: Server error: CUDA out of memory. Tried to allocate 2.55 GiB (GPU 0; 22.20 GiB total capacity; 13.26 GiB already allocated; 2.16 GiB free; 18.73 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\",\"error_type\":\"generation\"}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-27-12-53-49-511 in account 802376408542 for more information..\n",
      " - Waiting for 3 minutes to allow endpoint time to recover.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input_length': 100, 'max_new_tokens': 1793, 'concurrent_requests': 1},\n",
       " {'input_length': 1000, 'max_new_tokens': 1793, 'concurrent_requests': 1},\n",
       " {'input_length': 19199, 'max_new_tokens': 100, 'concurrent_requests': 1},\n",
       " {'input_length': 19199, 'max_new_tokens': 500, 'concurrent_requests': 1},\n",
       " {'input_length': 500, 'max_new_tokens': 100, 'concurrent_requests': 48},\n",
       " {'input_length': 1000, 'max_new_tokens': 100, 'concurrent_requests': 18}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from benchmarking.probe import run_probe\n",
    "\n",
    "\n",
    "run_probe(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = (\n",
    "    \"United Arab Emirate’s (UAE) Technology Innovation Institute (TII), the applied\" \\\n",
    "    \"research pillar of Abu Dhabi’s \" \\\n",
    "    \"Advanced Technology Research Council, has launched Falcon LLM, a foundational large language model\" \\\n",
    "    \" (LLM) with40 billion parameters. TII is a leading global research center dedicated to pushing the\" \\\n",
    "    \" frontiers of knowledge. TII’s team of scientists, researchers, and engineers work to deliver\" \\\n",
    "    \" discovery science and transformative technologies. TII’s work focuses on breakthroughs that \" \\\n",
    "    \"will future-proof our society. Trained on 1 trillion tokens, TII Falcon LLM boasts top-notch\" \\\n",
    "    \" performance while remaining incredibly cost-effective.Falcon-40B matches the performance of other \" \\\n",
    "    \"high-performing LLMs, and is the top-ranked open-source model in the public Hugging Face Open \" \\\n",
    "    \"LLM leaderboard. It’s available as open-source in two different sizes – Falcon-40B and Falcon-7B and\" \\\n",
    "    \" was built from scratch using data preprocessing and model training jobs built on Amazon SageMaker. \" \\\n",
    "    \"Open-sourcing Falcon 40B enables users to construct and customize AI tools that cater to unique\" \\\n",
    "    \" users needs, facilitating seamless integration and ensuring the long-term preservation of data \" \\\n",
    "    \"assets. The model weights are available to download, inspect and deploy anywhere. Starting June\" \\\n",
    "    \" 7th, both Falcon LLMs will also be available in Amazon SageMaker JumpStart, SageMaker’s machine\" \\\n",
    "    \" learning (ML) hub that offers pre-trained models, built-in algorithms, and pre-built solution \" \\\n",
    "    \"templates to help you quickly get started with ML. You can deploy and use the Falcon LLMs with a \" \\\n",
    "    \"few clicks in SageMaker Studio or programmatically through the SageMaker Python SDK. To deploy and \" \\\n",
    "    \"run inference against Falcon LLMs, refer to the Introduction to SageMaker JumpStart – Text\" \\\n",
    "    \" Generation with Falcon LLMs example notebook. Dr. Ebtesam Almazrouei, Executive Director–Acting\" \\\n",
    "    \" Chief AI Researcher of the AI-Cross Center Unit and Project Lead for LLM Projects at TII,\" \\\n",
    "    \" shares: “We proudly announce the official open-source release of Falcon-40B, the world’s\" \\\n",
    "    \" top-ranking open-source language model. Falcon-40B is an exceptional open-source model with 40B\" \\\n",
    "    \" parameters, specifically designed as a causal decoder-only model. It was trained on a vast dataset\" \\\n",
    "    \" of 1,000B tokens, including RefinedWeb enhanced with curated corpora. The model is made available\" \\\n",
    "    \" under the Apache 2.0 license, ensuring its accessibility and usability. Falcon-40B has surpassed\" \\\n",
    "    \" renowned models like LLaMA-65B, StableLM and MPT on the public leaderboard maintained by Hugging\" \\\n",
    "    \" Face. The architecture of Falcon-40B is optimized for inference, incorporating FlashAttention and\" \\\n",
    "    \" multiquery techniques.” “This step reflects our dedication to pushing the boundaries of AI \" \\\n",
    "    \"innovation and technology readiness level for community engagement, education, real-world\" \\\n",
    "    \" applications, and collaboration. Continues Dr Ebtesam. “By releasing Falcon-40B as an open-source\" \\\n",
    "    \" model, we provide researchers, entrepreneurs, and organizations with the opportunity to harness\" \\\n",
    "    \" its exceptional capabilities and drive advancements in AI-driven solutions from healthcare to space,\" \\\n",
    "    \" finance, manufacturing to biotech; the possibilities for AI-driven solutions are boundless.\" \\\n",
    "    \" To access Falcon-40B and explore its remarkable potential, please visit FalconLLM.tii.ae. Join us in\" \\\n",
    "    \" leveraging the power of Falcon-40B to shape the future of AI and revolutionize industries” In this\" \\\n",
    "    \" post, we dive deep with Dr. Almazrouei about Falcon LLM training on SageMaker, data curation, \" \\\n",
    "    \"optimization, performance, and next steps. A new generation of LLMs LLMs are software algorithms\" \\\n",
    "    \" trained to complete natural text sequences. Due to their size and the volume of training data\" \\\n",
    "    \" they interact with, LLMs have impressive text processing abilities, including summarization,\" \\\n",
    "    \" question answering, in-context learning, and more. In early 2020, research organizations across\" \\\n",
    "    \" the world set the emphasis on model size, observing that accuracy correlated with number of\" \\\n",
    "    \" parameters. For example, GPT-3 (2020) and BLOOM (2022) feature around 175 billion parameters,\" \\\n",
    "    \" Gopher (2021) has 230 billion parameters, and MT-NLG (2021) 530 billion parameters. In 2022,\" \\\n",
    "    \" Hoffman et al. observed that the current balance of compute between model parameters and dataset\" \\\n",
    "    \" size was suboptimal, and published empirical scaling laws suggesting that balancing the compute\" \\\n",
    "    \" budget towards smaller models trained on more data could lead to better performing models. They\" \\\n",
    "    \" implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much\" \\\n",
    "    \" bigger models. Summarize the article above.\"\n",
    ")\n",
    "payload = {\n",
    "    \"inputs\": inputs,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 50,\n",
    "    }\n",
    "}\n",
    "payloads = {\n",
    "    \"stress_test\": payload,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Model 'huggingface-llm-falcon-7b-instruct-bf16', Payload 'stress_test'): Begin throughput load test ...\n",
      "(Model 'huggingface-llm-falcon-7b-instruct-bf16', Payload 'stress_test'): Finished benchmarking load tests ...\n",
      "(Model 'huggingface-llm-falcon-7b-instruct-bf16'): Skipping cleaning up resources ...\n",
      "[{'Throughput': 0.31310795780782597, 'OutputSequenceWords': {'Average': 744.0, 'Minimum': 744, 'Maximum': 744, 'p50': 744.0, 'p90': 744.0, 'p95': 744.0}, 'WordThroughput': 232.9523206090225, 'SampleOutput': ' In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the compute budget towards smaller models trained on more data could lead to better performing models. They implemented their guidance in the 70B parameter Chinchilla (2022) model, that outperformed much bigger models. (Source: Hoffman et al. 2022) In 2022, Hoffman et al. observed that the current balance of compute between model parameters and dataset size was suboptimal, and published empirical scaling laws suggesting that balancing the', 'ModelID': 'huggingface-llm-falcon-7b-instruct-bf16', 'PayloadName': 'stress_test'}]\n"
     ]
    }
   ],
   "source": [
    "from benchmarking.runner import Benchmarker\n",
    "\n",
    "\n",
    "benchmarker = Benchmarker(payloads=payloads, max_concurrent_benchmarks=10, num_invocations=15, run_latency_load_test=False)\n",
    "metrics = benchmarker.run_single_predictor(model_id=model_id, predictor=predictor, clean_up=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for max_new_tokens=100\n",
      "(Model 'huggingface-llm-falcon-40b-instruct-bf16', Payload 'stress_test'): Begin throughput load test ...\n",
      "(Model 'huggingface-llm-falcon-40b-instruct-bf16'): Skipping cleaning up resources ...\n"
     ]
    },
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\"error\":\"Request failed during generation: Server error: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 22.20 GiB total capacity; 19.63 GiB already allocated; 5.12 MiB free; 20.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\",\"error_type\":\"generation\"}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-20-12-58-02-922 in account 802376408542 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning for max_new_tokens=\u001b[39m\u001b[39m{\u001b[39;00mmax_new_tokens\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m benchmarker \u001b[39m=\u001b[39m Benchmarker(payloads\u001b[39m=\u001b[39mpayloads, max_concurrent_benchmarks\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, num_invocations\u001b[39m=\u001b[39mnum_invocations, run_latency_load_test\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 20\u001b[0m metrics \u001b[39m=\u001b[39m benchmarker\u001b[39m.\u001b[39;49mrun_single_predictor(model_id\u001b[39m=\u001b[39;49mmodel_id, predictor\u001b[39m=\u001b[39;49mpredictor, clean_up\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     21\u001b[0m metrics[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mMaxNewTokens\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m max_new_tokens\n\u001b[1;32m     22\u001b[0m metrics[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mNumInvocations\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m num_invocations\n",
      "File \u001b[0;32m~/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/benchmarking/runner.py:49\u001b[0m, in \u001b[0;36mBenchmarker.run_single_predictor\u001b[0;34m(self, model_id, predictor, clean_up)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[39mfor\u001b[39;00m payload_name, payload \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpayloads\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 49\u001b[0m         metrics_payload \u001b[39m=\u001b[39m run_benchmarking_load_tests(\n\u001b[1;32m     50\u001b[0m             predictor\u001b[39m=\u001b[39;49mpredictor,\n\u001b[1;32m     51\u001b[0m             payload\u001b[39m=\u001b[39;49mpayload,\n\u001b[1;32m     52\u001b[0m             model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m     53\u001b[0m             payload_name\u001b[39m=\u001b[39;49mpayload_name,\n\u001b[1;32m     54\u001b[0m             num_invocations\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_invocations,\n\u001b[1;32m     55\u001b[0m             max_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_workers,\n\u001b[1;32m     56\u001b[0m             retry_wait_time\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretry_wait_time,\n\u001b[1;32m     57\u001b[0m             max_total_retry_time\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_total_retry_time,\n\u001b[1;32m     58\u001b[0m             run_latency_load_test\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_latency_load_test,\n\u001b[1;32m     59\u001b[0m             run_throughput_load_test\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_throughput_load_test,\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m         metrics\u001b[39m.\u001b[39mappend(metrics_payload)\n\u001b[1;32m     62\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/benchmarking/load_test.py:247\u001b[0m, in \u001b[0;36mrun_benchmarking_load_tests\u001b[0;34m(predictor, payload, model_id, payload_name, num_invocations, max_workers, retry_wait_time, max_total_retry_time, run_latency_load_test, run_throughput_load_test)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m run_throughput_load_test \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlogging_prefix(model_id,\u001b[39m \u001b[39mpayload_name)\u001b[39m}\u001b[39;00m\u001b[39m Begin throughput load test ...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m     statistics_throughput \u001b[39m=\u001b[39m run_load_test(predictor, payload, num_invocations, max_workers)\n\u001b[1;32m    248\u001b[0m     metrics[\u001b[39m\"\u001b[39m\u001b[39mThroughput\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m statistics_throughput\u001b[39m.\u001b[39mthroughput()\n\u001b[1;32m    249\u001b[0m     metrics[\u001b[39m\"\u001b[39m\u001b[39mOutputSequenceWords\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m statistics_throughput\u001b[39m.\u001b[39mget_client_statistics()[\u001b[39m\"\u001b[39m\u001b[39mOutputSequenceWords\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/benchmarking/load_test.py:134\u001b[0m, in \u001b[0;36mrun_load_test\u001b[0;34m(predictor, payload, num_invocations, max_workers)\u001b[0m\n\u001b[1;32m    131\u001b[0m     payloads \u001b[39m=\u001b[39m repeat(payload, num_invocations)\n\u001b[1;32m    132\u001b[0m     results \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39mmap(predict_once_and_collect_client_results, predictors, payloads)\n\u001b[0;32m--> 134\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(results)\n\u001b[1;32m    135\u001b[0m time_utc_end \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mutcnow()\n\u001b[1;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m BatchInvocationStatistics(time_utc_start, time_utc_end, num_invocations, results)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/concurrent/futures/_base.py:611\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    609\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    610\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39;49mpop()\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    612\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39mpop()\u001b[39m.\u001b[39mresult(end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/concurrent/futures/_base.py:432\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    431\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 432\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    434\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/concurrent/futures/_base.py:388\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_result\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m--> 388\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/concurrent/futures/thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/code/aws/amazon-sagemaker-examples/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-benchmarking/benchmarking/load_test.py:116\u001b[0m, in \u001b[0;36mpredict_once_and_collect_client_results\u001b[0;34m(predictor, payload)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform a single endpoint prediction and produce a PredictionResult.\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m time_utc_start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mutcnow()\n\u001b[0;32m--> 116\u001b[0m result \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mpredict(payload)\n\u001b[1;32m    117\u001b[0m time_utc_end \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mutcnow()\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m PredictionResult(time_utc_start, time_utc_end, payload, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/sagemaker/base_predictor.py:167\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m        as is.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_request_args(\n\u001b[1;32m    165\u001b[0m     data, initial_args, target_model, target_variant, inference_id\n\u001b[1;32m    166\u001b[0m )\n\u001b[0;32m--> 167\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49msagemaker_runtime_client\u001b[39m.\u001b[39;49minvoke_endpoint(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mrequest_args)\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/py38-huggingface-test/lib/python3.8/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (424) from primary with message \"{\"error\":\"Request failed during generation: Server error: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 22.20 GiB total capacity; 19.63 GiB already allocated; 5.12 MiB free; 20.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\",\"error_type\":\"generation\"}\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/hf-llm-falcon-40b-instruct-bf16-2023-06-20-12-58-02-922 in account 802376408542 for more information."
     ]
    }
   ],
   "source": [
    "from benchmarking.runner import Benchmarker\n",
    "\n",
    "results = []\n",
    "max_new_tokens = 100\n",
    "num_invocations = 10\n",
    "# for max_new_tokens in range(800, 1800, 100):\n",
    "for num_invocations in range(1,30):\n",
    "    payload = {\n",
    "        \"inputs\": inputs,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "    }\n",
    "    payloads = {\n",
    "        \"stress_test\": payload,\n",
    "    }\n",
    "\n",
    "    print(f\"Running for max_new_tokens={max_new_tokens}\")\n",
    "    benchmarker = Benchmarker(payloads=payloads, max_concurrent_benchmarks=10, num_invocations=num_invocations, run_latency_load_test=False)\n",
    "    metrics = benchmarker.run_single_predictor(model_id=model_id, predictor=predictor, clean_up=False)\n",
    "    metrics[0]['MaxNewTokens'] = max_new_tokens\n",
    "    metrics[0]['NumInvocations'] = num_invocations\n",
    "    results.extend(metrics)\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "NumInvocations=%{x}<br>WordThroughput=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22
         ],
         "xaxis": "x",
         "y": [
          20.111730577591903,
          36.113055225508226,
          50.01884512827015,
          62.87304581607554,
          73.50570663106367,
          83.51208710036231,
          92.47361524947594,
          100.1511259331643,
          107.991178185444,
          114.37060166507489,
          120.22387193525337,
          124.38956768639346,
          131.63180800637994,
          135.7704261415711,
          140.03228500212154,
          142.5912546373538,
          134.84312951353022,
          148.3382288044055,
          150.42273919027517,
          153.99607895617552,
          156.36770510833682,
          162.52317541983678
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "throughput vs. invocations for huggingface-llm-falcon-7b-instruct-bf16"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "NumInvocations"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "WordThroughput"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"00aa610d-f201-4723-a672-ac70a63b7e0b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"00aa610d-f201-4723-a672-ac70a63b7e0b\")) {                    Plotly.newPlot(                        \"00aa610d-f201-4723-a672-ac70a63b7e0b\",                        [{\"hovertemplate\":\"NumInvocations=%{x}\\u003cbr\\u003eWordThroughput=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\"xaxis\":\"x\",\"y\":[20.111730577591903,36.113055225508226,50.01884512827015,62.87304581607554,73.50570663106367,83.51208710036231,92.47361524947594,100.1511259331643,107.991178185444,114.37060166507489,120.22387193525337,124.38956768639346,131.63180800637994,135.7704261415711,140.03228500212154,142.5912546373538,134.84312951353022,148.3382288044055,150.42273919027517,153.99607895617552,156.36770510833682,162.52317541983678],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"NumInvocations\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"WordThroughput\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"throughput vs. invocations for huggingface-llm-falcon-7b-instruct-bf16\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('00aa610d-f201-4723-a672-ac70a63b7e0b');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(results)\n",
    "# display(df)\n",
    "fig = px.line(df, x=\"NumInvocations\", y=\"WordThroughput\", title=f\"throughput vs. invocations for {model_id}\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-huggingface-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
